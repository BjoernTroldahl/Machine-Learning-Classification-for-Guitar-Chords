{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "796951f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d869d",
   "metadata": {},
   "source": [
    "### Init model, load saved weights and read class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "faed6315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Am' 'Background' 'C' 'Dm' 'Em' 'F' 'G']\n"
     ]
    }
   ],
   "source": [
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.is_dir()]))\n",
    "print(class_names)\n",
    "\n",
    "model_input_shape = (224,224,3)\n",
    "\n",
    "def loadCNNModel(saved_weights_path):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create model -> UPDATE if architecture changes\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (5,5), activation='relu', input_shape=model_input_shape), # filters, kernel_size\n",
    "        Conv2D(32, (5,5), activation='relu'),\n",
    "        MaxPool2D(), #pool_size=2, padding=\"valid\"\n",
    "        Conv2D(32, (5,5), activation='relu'),\n",
    "        Conv2D(32, (5,5), activation='relu'),\n",
    "        MaxPool2D(), #2\n",
    "        Flatten(),\n",
    "        Dense(7, activation='softmax') # 7 is number of classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Load the model from the saved Keras file\n",
    "    model.load_weights(saved_weights_path)\n",
    "    \n",
    "    # Keras builds the GPU function the first time you call predict(). \n",
    "    # That way, if you never call predict, you save some time and resources. \n",
    "    # However, the first time you call predict is slightly slower than every other time.\n",
    "    model.predict(np.array([np.ones(model_input_shape)]));\n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7db771",
   "metadata": {},
   "source": [
    "## Using Camera as model's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "928039f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crw-rw---- 1 root video 81, 0 Nov 14 20:55 /dev/video0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "!ls -ltrh /dev/video* # linux command to list available cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90680e1e",
   "metadata": {},
   "source": [
    "#### Create the camera object\n",
    "Remember to change device if incorrect for your system (based on out put of camera listing /dev/video\\<camera_no>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7fa64239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VideoCapture object to access the camera\n",
    "# If \"0\" doesn't work for, try uncommenting the long init\n",
    "def initCameraObject():\n",
    "    # return cv2.VideoCapture(\"v4l2src device=/dev/video0 ! video/x-raw,format=YUY2,width=640,height=480,framerate=30/1 ! nvvidconv ! video/x-raw(memory:NVMM) ! nvvidconv ! video/x-raw, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink drop=1 \", cv2.CAP_GSTREAMER)\n",
    "    return cv2.VideoCapture(0)\n",
    "\n",
    "# Predict \n",
    "def predict(model, image):\n",
    "    if(image.shape[0:2] != model_input_shape[0:2]): # check size of input image\n",
    "        image = cv2.resize(image, model_input_shape[0:2], interpolation = cv2.INTER_AREA) \n",
    "        \n",
    "    with session.as_default():\n",
    "            with graph.as_default():\n",
    "                predictions = model.predict( np.array([image]));\n",
    "                \n",
    "    pred_index = np.argmax(predictions);\n",
    "    return class_names[pred_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e447e",
   "metadata": {},
   "source": [
    "### How to send UDP from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "094ce728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDP target IP: 192.168.0.94\n",
      "UDP target port: 14550\n",
      "message: Hello UDP\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "UDP_IP = \"192.168.0.94\" # Replace by destination's IP\n",
    "UDP_PORT = 14550 # Replace with desired Port \n",
    "MESSAGE = \"Hello UDP\" \n",
    "\n",
    "print(\"UDP target IP:\", UDP_IP)\n",
    "print(\"UDP target port:\", UDP_PORT)\n",
    "print(\"message:\", MESSAGE)\n",
    "\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM); # UDP\n",
    "sock.sendto(bytes(MESSAGE, \"utf-8\"), (UDP_IP, UDP_PORT));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475c0bd",
   "metadata": {},
   "source": [
    "### Video update callback\n",
    "\n",
    "Define callback for camera. Every step in loop we fetch the image, rescale it to shape required by our model (if needed) and run a prediction. The prediction is diplayed in the bottom left corner of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "60b619b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affa515e2aa947f794df440ca80767c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop button widget\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger',\n",
    "    tooltip='Description',\n",
    "    icon='square'\n",
    ")\n",
    "\n",
    "# Display function\n",
    "def view(model, button):\n",
    "    cap = initCameraObject(); # init camera\n",
    "    display_handle = display(None, display_id=True) # display inside Jupyter Notebook\n",
    "    \n",
    "    while True: # video \"callback\"\n",
    "        ret, frame = cap.read() # read the frame\n",
    "        \n",
    "        if not ret or stopButton.value == True: # if frame can't be read or user clicked the \"Stop\" button\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "        \n",
    "        prediction = predict(model, frame)\n",
    "        # Send predicted class by UDP to computer running our plugin\n",
    "        sock.sendto(bytes(prediction, \"utf-8\"), (UDP_IP, UDP_PORT))\n",
    "        \n",
    "        # Display predicted class\n",
    "        cv2.putText(frame, prediction, (10, frame.shape[0] - 10), font, 3, (0, 255, 0), 2, cv2.LINE_AA, False)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "           \n",
    "            \n",
    "# Run\n",
    "# ================\n",
    "display(stopButton) # show stop button\n",
    "\n",
    "cnn_model = loadCNNModel('Models/model_1_weights.h5')\n",
    "\n",
    "session = K.get_session()\n",
    "graph = tf.get_default_graph()\n",
    "graph.finalize() # finalize\n",
    "\n",
    "thread = threading.Thread(target=view, args=(cnn_model, stopButton,))\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

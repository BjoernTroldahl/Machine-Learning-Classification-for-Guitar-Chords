{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfcd08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5a29c",
   "metadata": {},
   "source": [
    "### Init model, load saved weights and read class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d8e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'Background' 'C' 'D' 'E' 'F' 'G']\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"Dataset/Train\"\n",
    "data_dir = pathlib.Path(train_dir)\n",
    "\n",
    "class_names = np.array(sorted([item.name for item in data_dir.glob('*') if item.is_dir()]))\n",
    "print(class_names)\n",
    "\n",
    "model_input_shape = (224,224,3)\n",
    "\n",
    "def loadCNNModel(saved_weights_path):\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create model -> UPDATE if architecture changes\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (5,5), activation='relu', input_shape=model_input_shape), # filters, kernel_size\n",
    "        Conv2D(32, (5,5), activation='relu'),\n",
    "        MaxPool2D(), #pool_size=2, padding=\"valid\"\n",
    "        Conv2D(32, (5,5), activation='relu'),\n",
    "        Conv2D(32, (5,5), activation='relu'),\n",
    "        MaxPool2D(), #2\n",
    "        Flatten(),\n",
    "        Dense(7, activation='softmax') # 7 is number of classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Load the model from the saved Keras file\n",
    "    model.load_weights(saved_weights_path)\n",
    "    \n",
    "    # Keras builds the GPU function the first time you call predict(). \n",
    "    # That way, if you never call predict, you save some time and resources. \n",
    "    # However, the first time you call predict is slightly slower than every other time.\n",
    "    model.predict(np.array([np.ones(model_input_shape)]));\n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057759b9",
   "metadata": {},
   "source": [
    "## Using Camera as model's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7623fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crw-rw---- 1 root video 81, 0 Dec  2 12:10 /dev/video0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "!ls -ltrh /dev/video* # linux command to list available cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4488db",
   "metadata": {},
   "source": [
    "#### Create the camera object\n",
    "Remember to change device if incorrect for your system (based on out put of camera listing /dev/video\\<camera_no>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28df634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VideoCapture object to access the camera\n",
    "# If \"0\" doesn't work for, try uncommenting the long init\n",
    "def initCameraObject():\n",
    "    # return cv2.VideoCapture(\"v4l2src device=/dev/video0 ! video/x-raw,format=YUY2,width=640,height=480,framerate=30/1 ! nvvidconv ! video/x-raw(memory:NVMM) ! nvvidconv ! video/x-raw, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! appsink drop=1 \", cv2.CAP_GSTREAMER)\n",
    "    return cv2.VideoCapture(0)\n",
    "\n",
    "# Predict \n",
    "def predict(model, image):\n",
    "    if(image.shape[0:2] != model_input_shape[0:2]): # check size of input image\n",
    "        image = cv2.resize(image, model_input_shape[0:2], interpolation = cv2.INTER_AREA) \n",
    "        \n",
    "    with session.as_default():\n",
    "            with graph.as_default():\n",
    "                predictions = model.predict( np.array([image]));\n",
    "     \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865b5b6",
   "metadata": {},
   "source": [
    "### How to send UDP from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23d50d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDP target IP: 192.168.0.91\n",
      "UDP target port: 1108\n",
      "message: Hello UDP\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "UDP_IP = \"192.168.0.91\" # Replace by destination's IP\n",
    "UDP_PORT = 1108 # Replace with desired Port \n",
    "MESSAGE = \"Hello UDP\" \n",
    "\n",
    "print(\"UDP target IP:\", UDP_IP)\n",
    "print(\"UDP target port:\", UDP_PORT)\n",
    "print(\"message:\", MESSAGE)\n",
    "\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM); # UDP\n",
    "\n",
    "chord_mappings = [\"Background\",\n",
    "                  \"A\",\"Am\",\"A#\",\"A#m\",\n",
    "                  \"B\",\"Bm\",\n",
    "                  \"C\",\"Cm\", \"C#\",\"C#m\", \n",
    "                  \"D\",\"Dm\",\"D#\",\"D#m\",\n",
    "                  \"E\",\"Em\",\n",
    "                  \"F\",\"Fm\",\"F#\",\"F#m\",\n",
    "                  \"G\",\"Gm\",\"G#\",\"G#m\"];\n",
    "\n",
    "\n",
    "lastPrediction = \"\"\n",
    "matchCount = 0\n",
    "sendThreshold = 5\n",
    "\n",
    "def sendPredictionUDP(prediction):\n",
    "    global matchCount \n",
    "    global lastPrediction\n",
    "    if prediction == lastPrediction:\n",
    "        matchCount += 1\n",
    "        \n",
    "    lastPrediction = prediction;\n",
    "    \n",
    "    if matchCount < sendThreshold:\n",
    "        return\n",
    " \n",
    "    chord_number = chord_mappings.index(prediction)\n",
    "    sock.sendto(chord_number.to_bytes(length=1, byteorder='little', signed=False), (UDP_IP, UDP_PORT))\n",
    "    matchCount = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384575f",
   "metadata": {},
   "source": [
    "### Video update callback\n",
    "\n",
    "Define callback for camera. Every step in loop we fetch the image, rescale it to shape required by our model (if needed) and run a prediction. The prediction is diplayed in the bottom left corner of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41ceab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f18f4d83d1b44bdb954bb550fcec0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, button_style='danger', description='Stop', icon='square', tooltip='Description')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop button widget\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger',\n",
    "    tooltip='Description',\n",
    "    icon='square'\n",
    ")\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "start_time = 0\n",
    "\n",
    "# Display function\n",
    "def view(model, button):\n",
    "    cap = initCameraObject(); # init camera\n",
    "    display_handle = display(None, display_id=True) # display inside Jupyter Notebook\n",
    "    \n",
    "    while True: # video \"callback\"\n",
    "        global start_time \n",
    "        time = timer() - start_time\n",
    "        start_time = timer()\n",
    "        ret, frame = cap.read() # read the frame\n",
    "        \n",
    "        if not ret or stopButton.value == True: # if frame can't be read or user clicked the \"Stop\" button\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n",
    "        \n",
    "        predictions = predict(model, frame) # get probabilities for each class\n",
    "        pred_index = np.argmax(predictions); # max probablity index\n",
    "        prediction = class_names[pred_index] # class name at that index is our prediction\n",
    "\n",
    "        # Send predicted class by UDP to computer running our plugin\n",
    "        sendPredictionUDP(prediction)\n",
    "        \n",
    "        # Display predicted class\n",
    "        cv2.putText(frame, (\"Chord: {:s} | Prediction Confidence: {:3.2f}%\").format(prediction, np.max(predictions) * 100),\n",
    "                    (10, frame.shape[0] - 10), font, 1, (0, 255, 0), 2, cv2.LINE_AA, False)\n",
    "        cv2.putText(frame, \"FPS: \" + str(round(1 / time,1)), (10, 50), font, 1, (0, 255, 0), 2, cv2.LINE_AA, False)\n",
    "        \n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        \n",
    "           \n",
    "            \n",
    "# Run\n",
    "# ================\n",
    "display(stopButton) # show stop button\n",
    "\n",
    "cnn_model = loadCNNModel('Models/model_1_weights.h5')\n",
    "\n",
    "session = K.get_session()\n",
    "graph = tf.get_default_graph()\n",
    "graph.finalize() # finalize\n",
    "\n",
    "thread = threading.Thread(target=view, args=(cnn_model, stopButton,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3f315",
   "metadata": {},
   "source": [
    "### TIMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e20bcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_timing = loadCNNModel('Models/model_1_weights.h5')\n",
    "N = 1000\n",
    "\n",
    "start = timer()\n",
    "\n",
    "for i in range(N):\n",
    "    prediction = model_timing.predict(np.array([np.ones(model_input_shape)]));\n",
    "    \n",
    "end = timer()\n",
    "\n",
    "avg_time = (end - start) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf16c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time of a predicition on N=1000 test samples: 0.048650\n",
      "Predicted FPS: 20.554812\n"
     ]
    }
   ],
   "source": [
    "print(\"Average time of a predicition on N={:d} test samples: {:3f}\".format(N, avg_time)) # Average time of prediction\n",
    "print(\"Predicted FPS: {:f}\".format(1 / avg_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
